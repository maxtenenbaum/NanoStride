{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ee36f0f",
   "metadata": {},
   "source": [
    "We need to fix the waveform generation so that it isn't just 1 or 0 for a pixel. We need the waveform to be set up for temporal accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "38fdd02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import nifgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "dc542eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image = r\"C:\\Users\\max\\Desktop\\NanoStride\\skull_slices\\skull00011.png\"\n",
    "image_path = r\"C:\\Users\\max\\Desktop\\NanoStride\\skull_slices\"\n",
    "sampling_rate = 10_000_000\n",
    "waveforms = []\n",
    "\n",
    "for i in os.listdir(image_path):\n",
    "    image = os.path.join(image_path, i)\n",
    "    # Calculate number of pixels in the first row and save the variable\n",
    "    img = Image.open(image).convert('L')\n",
    "    binary = (np.array(img) > 128).astype(np.uint8)\n",
    "    pixels_per_row = binary.shape[1]\n",
    "\n",
    "    # Calculate amount of time for each pixel\n",
    "    scan_frequency = 7911.78\n",
    "    time_per_row = (1/scan_frequency)/2\n",
    "    time_per_pixel = time_per_row/pixels_per_row\n",
    "    datapoints_per_pixel = np.floor(time_per_pixel * sampling_rate)\n",
    "\n",
    "    # Create black, white, gap array\n",
    "    black = np.ones(int(datapoints_per_pixel))*(-1)\n",
    "    white = np.ones(int(datapoints_per_pixel))\n",
    "    gap = np.ones(int(datapoints_per_pixel * pixels_per_row))*-1\n",
    "\n",
    "    for row_index in range(pixels_per_row):\n",
    "        row = binary[row_index]\n",
    "        for column in row:\n",
    "            if column == 0:\n",
    "                waveforms.append(black)\n",
    "            else:\n",
    "                waveforms.append(white)\n",
    "        waveforms.append(gap)\n",
    "\n",
    "waveform_list = np.concatenate(waveforms)\n",
    "waveform_numpy = np.array(waveform_list).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8dd229bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples per row (half-cycle): 632\n",
      "skull00000.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00001.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00002.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00003.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00004.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00005.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00006.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00007.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00008.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00009.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00010.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00011.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00012.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00013.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00014.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00015.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00016.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00017.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00018.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00019.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00020.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00021.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00022.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00023.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00024.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00025.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00026.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00027.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00028.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00029.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00030.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00031.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00032.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00033.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00034.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00035.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00036.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00037.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00038.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00039.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00040.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00041.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00042.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00043.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00044.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00045.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00046.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00047.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00048.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00049.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00050.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00051.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00052.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00053.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00054.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00055.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00056.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00057.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00058.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00059.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00060.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00061.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00062.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00063.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00064.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00065.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00066.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00067.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00068.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00069.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00070.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00071.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00072.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00073.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00074.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00075.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00076.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00077.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00078.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00079.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00080.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00081.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00082.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00083.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00084.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00085.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00086.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00087.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00088.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00089.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00090.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00091.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00092.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00093.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00094.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00095.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00096.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00097.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00098.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00099.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00100.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00101.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00102.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00103.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00104.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00105.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00106.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00107.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00108.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00109.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00110.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00111.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00112.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00113.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00114.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00115.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00116.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00117.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00118.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00119.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00120.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00121.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00122.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00123.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00124.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00125.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00126.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00127.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00128.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00129.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00130.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00131.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00132.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00133.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00134.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00135.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00136.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00137.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00138.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00139.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00140.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00141.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00142.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00143.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00144.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00145.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00146.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00147.png: 300 pixels → 2 samples/pixel + 32 extra\n",
      "skull00148.png: 300 pixels → 2 samples/pixel + 32 extra\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Path to image directory\n",
    "image_path = r\"C:\\Users\\max\\Desktop\\NanoStride\\skull_slices\"\n",
    "\n",
    "# Constants\n",
    "sampling_rate = 10_000_000  # 10 MS/s\n",
    "scan_frequency = 7911.48    # Resonant mirror frequency (Hz)\n",
    "waveforms = []\n",
    "\n",
    "# Time per scanline (bottom half of cycle)\n",
    "time_per_row = 1 / (2 * scan_frequency)  # seconds per row (~63.2 µs)\n",
    "samples_per_row = int(round(time_per_row * sampling_rate))  # e.g. 632 samples\n",
    "\n",
    "print(f\"Samples per row (half-cycle): {samples_per_row}\")\n",
    "\n",
    "# Process each image\n",
    "for filename in os.listdir(image_path):\n",
    "    image = os.path.join(image_path, filename)\n",
    "    img = Image.open(image).convert('L')\n",
    "    binary = (np.array(img) > 128).astype(np.uint8)\n",
    "    \n",
    "    n_rows = binary.shape[0]\n",
    "    pixels_per_row = binary.shape[1]\n",
    "\n",
    "    base_samples_per_pixel = samples_per_row // pixels_per_row\n",
    "    remainder = samples_per_row % pixels_per_row\n",
    "\n",
    "    print(f\"{filename}: {pixels_per_row} pixels → {base_samples_per_pixel} samples/pixel + {remainder} extra\")\n",
    "\n",
    "    # Create the waveform row by row\n",
    "    for row in binary:\n",
    "        for i, pixel in enumerate(row):\n",
    "            extra = 1 if i < remainder else 0\n",
    "            n_samples = base_samples_per_pixel + extra\n",
    "            value = 1.0 if pixel else -1.0\n",
    "            waveforms.append(np.ones(n_samples, dtype=np.float64) * value)\n",
    "\n",
    "        # Add a \"gap\" for the top half of the mirror swing (same length as row)\n",
    "        waveforms.append(np.ones(samples_per_row, dtype=np.float64) * -1.0)\n",
    "\n",
    "# Final waveform\n",
    "waveform_numpy = np.concatenate(waveforms).astype(np.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "6bc1c140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "452006400"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waveform_numpy.nbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9ad4404a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max mem: 83886080 bytes\n",
      "Chunk size: 8388608 bytes\n",
      "Elements per chunk: 1048576\n",
      "Total chunks needed: 54\n",
      "Generated 54 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Max allocation space in bytes\n",
    "max_mem = 80 * 1024 * 1024  # 160 MiB = 167_772_160 bytes\n",
    "target_chunk = max_mem // 10  # Ideal chunk size ~16.7 MiB\n",
    "\n",
    "# Find best chunk size that evenly divides max_mem\n",
    "for chunk_size in range(target_chunk, 0, -1):\n",
    "    if max_mem % chunk_size == 0:\n",
    "        break  # Found the best chunk size\n",
    "\n",
    "# Now compute how many elements that is\n",
    "bytes_per_element = waveform_numpy.itemsize\n",
    "elements_per_chunk = chunk_size // bytes_per_element\n",
    "\n",
    "print(f\"Max mem: {max_mem} bytes\")\n",
    "print(f\"Chunk size: {chunk_size} bytes\")\n",
    "print(f\"Elements per chunk: {elements_per_chunk}\")\n",
    "print(f\"Total chunks needed: {int(np.ceil(waveform_numpy.nbytes / chunk_size))}\")\n",
    "\n",
    "# Create the chunked list of numpy arrays\n",
    "waveform_chunks = [\n",
    "    waveform_numpy[i : i + elements_per_chunk]\n",
    "    for i in range(0, len(waveform_numpy), elements_per_chunk)\n",
    "]\n",
    "\n",
    "print(f\"Generated {len(waveform_chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "2c01eeb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Chunk 10\n",
      "Writing Chunk 11\n",
      "Writing Chunk 12\n",
      "Writing Chunk 13\n",
      "Writing Chunk 14\n",
      "Writing Chunk 15\n",
      "Writing Chunk 16\n",
      "Writing Chunk 17\n",
      "Writing Chunk 18\n",
      "Writing Chunk 19\n",
      "Writing Chunk 20\n",
      "Writing Chunk 21\n",
      "Writing Chunk 22\n",
      "Writing Chunk 23\n",
      "Writing Chunk 24\n",
      "Writing Chunk 25\n",
      "Writing Chunk 26\n",
      "Writing Chunk 27\n",
      "Writing Chunk 28\n",
      "Writing Chunk 29\n",
      "Writing Chunk 30\n",
      "Writing Chunk 31\n",
      "Writing Chunk 32\n",
      "Writing Chunk 33\n",
      "Writing Chunk 34\n",
      "Writing Chunk 35\n",
      "Writing Chunk 36\n",
      "Writing Chunk 37\n",
      "Writing Chunk 38\n",
      "Writing Chunk 39\n",
      "Writing Chunk 40\n",
      "Writing Chunk 41\n",
      "Writing Chunk 42\n",
      "Writing Chunk 43\n",
      "Writing Chunk 44\n",
      "Writing Chunk 45\n",
      "Writing Chunk 46\n",
      "Writing Chunk 47\n",
      "Writing Chunk 48\n",
      "Writing Chunk 49\n",
      "Writing Chunk 50\n",
      "Writing Chunk 51\n",
      "Writing Chunk 52\n",
      "Writing Chunk 53\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "with nifgen.Session(\"Dev1\") as session:\n",
    "    session.output_mode = nifgen.OutputMode.ARB\n",
    "    session.arb_sample_rate = 10_000_000\n",
    "\n",
    "    #80 MiB allocation\n",
    "    waveform_handle = session.allocate_waveform(max_mem)\n",
    "    session.streaming_waveform_handle = waveform_handle\n",
    "\n",
    "    # Write 10, 16MB chunks\n",
    "    for i in range(10):\n",
    "        session.write_waveform(waveform_handle, waveform_chunks[i])\n",
    "\n",
    "    # Configure trigger\n",
    "    session.start_trigger_type = nifgen.StartTriggerType.DIGITAL_EDGE\n",
    "    session.digital_edge_start_trigger_source = \"/Dev1/PFI1\"\n",
    "    session.digital_edge_start_trigger_edge = nifgen.StartTriggerDigitalEdgeEdge.RISING\n",
    "    session.trigger_mode = nifgen.TriggerMode.SINGLE\n",
    "\n",
    "    # Initiate\n",
    "    session.initiate()\n",
    "\n",
    "    # Write the rest of the chunks\n",
    "    for i in range(10, len(waveform_chunks)):\n",
    "        print(f'Writing Chunk {i}')\n",
    "        session.write_waveform(waveform_handle, waveform_chunks[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "673ecb57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_mem / chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ec27ad07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "83886080 / chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8b2002cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "83886080 / 1024 / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1f0080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bidirectional_waveforms(image_dir, serpentine=True):\n",
    "    waveforms = []\n",
    "\n",
    "    image_files = sorted([\n",
    "        f for f in os.listdir(image_dir)\n",
    "        if f.endswith(\".png\")\n",
    "    ])\n",
    "\n",
    "    for i, filename in enumerate(image_files):\n",
    "        path = os.path.join(image_dir, filename)\n",
    "        img = Image.open(path).convert(\"L\")\n",
    "        binary = (np.array(img) > 128).astype(np.uint8)\n",
    "        for row_index in range(binary.shape[0]):\n",
    "            row = binary[row_index]\n",
    "            for column in row:\n",
    "                if column == 0:\n",
    "                    waveforms.append(black)\n",
    "                else:\n",
    "                    waveforms.append(white)\n",
    "        return np.concatenate(waveforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb2773bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 149 image(s) from C:\\Users\\max\\Desktop\\NanoStride\\skull_slices...\n",
      "Total samples generated for waveform: 149000000\n",
      "Waveform chunked into 72 segments.\n",
      "Generated 72 chunks from images.\n",
      "  Chunk 1 shape: (2097152,), Dtype: float64, Size (MB): 16.00\n",
      "  Chunk 1 samples % 64 = 0\n",
      "  Chunk 2 shape: (2097152,), Dtype: float64, Size (MB): 16.00\n",
      "  Chunk 2 samples % 64 = 0\n",
      "  Chunk 3 shape: (2097152,), Dtype: float64, Size (MB): 16.00\n",
      "  Chunk 3 samples % 64 = 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image # Pillow library for image processing\n",
    "\n",
    "def create_bidirectional_waveforms(\n",
    "    image_dir: str,\n",
    "    serpentine: bool = True,\n",
    "    samples_per_channel_increment: int = 64\n",
    ") -> list[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generates a waveform from a sequence of binary images, outputting it\n",
    "    as a list of 16 MB NumPy array chunks of float64 data. Each chunk's\n",
    "    sample count and the total waveform's sample count will be an integer\n",
    "    multiple of samples_per_channel_increment.\n",
    "\n",
    "    Args:\n",
    "        image_dir (str): The directory containing the PNG image files.\n",
    "        serpentine (bool): If True, odd-numbered rows will be read in reverse\n",
    "                           order (left-to-right, then right-to-left, etc.).\n",
    "        samples_per_channel_increment (int): The required sample increment for\n",
    "                                             hardware compatibility (e.g., 64).\n",
    "                                             The total samples and each chunk's\n",
    "                                             samples will be a multiple of this.\n",
    "\n",
    "    Returns:\n",
    "        list[np.ndarray]: A list of NumPy arrays, where each array is a chunk\n",
    "                          of the generated waveform with dtype float64.\n",
    "                          Each chunk will be approximately 16 MB and its sample\n",
    "                          count will be a multiple of the increment.\n",
    "                          Returns an empty list if no images are found or an\n",
    "                          error occurs.\n",
    "    \"\"\"\n",
    "    # Define black and white levels for float64 output, normalized between -1 and 1\n",
    "    black_level = -1.0\n",
    "    white_level = 1.0\n",
    "\n",
    "    all_samples = []\n",
    "\n",
    "    try:\n",
    "        image_files = sorted([\n",
    "            f for f in os.listdir(image_dir)\n",
    "            if f.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".bmp\")) # Support common image formats\n",
    "        ])\n",
    "\n",
    "        if not image_files:\n",
    "            print(f\"No image files found in directory: {image_dir}\")\n",
    "            return []\n",
    "\n",
    "        print(f\"Processing {len(image_files)} image(s) from {image_dir}...\")\n",
    "\n",
    "        for i, filename in enumerate(image_files):\n",
    "            path = os.path.join(image_dir, filename)\n",
    "            try:\n",
    "                # Open image, convert to grayscale, then to binary (0 or 255)\n",
    "                img = Image.open(path).convert(\"L\")\n",
    "                binary_array = (np.array(img) > 128).astype(np.uint8) # Convert to 0 or 1\n",
    "\n",
    "                for row_index in range(binary_array.shape[0]):\n",
    "                    row = binary_array[row_index]\n",
    "                    # Apply serpentine logic if enabled for odd rows\n",
    "                    if serpentine and row_index % 2 == 1: # Odd rows (0-indexed)\n",
    "                        row = row[::-1] # Reverse row for serpentine scanning\n",
    "\n",
    "                    for column_value in row:\n",
    "                        if column_value == 0: # Black pixel\n",
    "                            all_samples.append(black_level)\n",
    "                        else: # White pixel\n",
    "                            all_samples.append(white_level)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {filename}: {e}\")\n",
    "                continue # Skip to next image\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Directory not found: {image_dir}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during image loading: {e}\")\n",
    "        return []\n",
    "\n",
    "    if not all_samples:\n",
    "        print(\"No samples generated from images. Returning empty list.\")\n",
    "        return []\n",
    "\n",
    "    # Convert the list of samples to a NumPy array of float64\n",
    "    full_waveform = np.array(all_samples, dtype=np.float64)\n",
    "\n",
    "    # --- Ensure total samples are a multiple of samples_per_channel_increment ---\n",
    "    num_samples_raw = full_waveform.shape[0]\n",
    "    num_samples = (num_samples_raw + samples_per_channel_increment - 1) // samples_per_channel_increment * samples_per_channel_increment\n",
    "\n",
    "    # Pad the waveform with the black_level if necessary to meet the aligned total samples\n",
    "    if num_samples > num_samples_raw:\n",
    "        padding_needed = num_samples - num_samples_raw\n",
    "        full_waveform = np.pad(full_waveform, (0, padding_needed), 'constant', constant_values=black_level)\n",
    "        print(f\"Padded waveform from {num_samples_raw} to {num_samples} samples for alignment.\")\n",
    "\n",
    "    print(f\"Total samples generated for waveform: {num_samples}\")\n",
    "\n",
    "    # --- Chunking logic (16MB chunks of float64) ---\n",
    "    chunk_size_mb = 16\n",
    "    bytes_per_sample = 8 # float64 is 8 bytes\n",
    "    chunk_size_bytes = chunk_size_mb * 1024 * 1024\n",
    "    chunk_size_samples_raw = chunk_size_bytes // bytes_per_sample\n",
    "\n",
    "    # Ensure the desired chunk size for each segment is a multiple of the increment\n",
    "    aligned_chunk_size_samples = (chunk_size_samples_raw + samples_per_channel_increment - 1) // samples_per_channel_increment * samples_per_channel_increment\n",
    "\n",
    "    waveform_chunks = []\n",
    "    start_index = 0\n",
    "    while start_index < num_samples:\n",
    "        end_index = min(start_index + aligned_chunk_size_samples, num_samples)\n",
    "        current_chunk = full_waveform[start_index:end_index]\n",
    "\n",
    "        # This check should ideally pass if num_samples and aligned_chunk_size_samples are correct\n",
    "        if current_chunk.shape[0] % samples_per_channel_increment != 0:\n",
    "            print(f\"Error: A chunk of size {current_chunk.shape[0]} was generated which is not a multiple of the increment {samples_per_channel_increment}. This should not happen with current logic.\")\n",
    "\n",
    "        waveform_chunks.append(current_chunk)\n",
    "        start_index = end_index\n",
    "\n",
    "    print(f\"Waveform chunked into {len(waveform_chunks)} segments.\")\n",
    "    return waveform_chunks\n",
    "\n",
    "# Example Usage (requires a directory named 'images' with PNG files)\n",
    "# import os\n",
    "# # Create a dummy image directory and files for testing if they don't exist\n",
    "# if not os.path.exists(\"test_images\"):\n",
    "#     os.makedirs(\"test_images\")\n",
    "#     # Create a simple 10x10 black and white image\n",
    "#     dummy_img_data = np.random.randint(0, 2, size=(10, 10), dtype=np.uint8) * 255\n",
    "#     dummy_img = Image.fromarray(dummy_img_data, 'L')\n",
    "#     dummy_img.save(\"test_images/dummy_image_01.png\")\n",
    "#     dummy_img.save(\"test_images/dummy_image_02.png\")\n",
    "#     print(\"Created dummy images in 'test_images' directory for demonstration.\")\n",
    "\n",
    "waveform_chunks_float64 = create_bidirectional_waveforms(image_dir=r\"C:\\Users\\max\\Desktop\\NanoStride\\skull_slices\", serpentine=True)\n",
    "if waveform_chunks_float64:\n",
    "    print(f\"Generated {len(waveform_chunks_float64)} chunks from images.\")\n",
    "    for i, chunk in enumerate(waveform_chunks_float64[:3]):\n",
    "        print(f\"  Chunk {i+1} shape: {chunk.shape}, Dtype: {chunk.dtype}, Size (MB): {chunk.nbytes / (1024*1024):.2f}\")\n",
    "        print(f\"  Chunk {i+1} samples % {64} = {chunk.shape[0] % 64}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b5f622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to save as a binary waveform now\n",
    "waveform_normalized = correct_waveform.astype(np.float64) * 2 -1\n",
    "#waveform_normalized.astype('<f8').tofile(f\"correct_waveform.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e72e7302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83886080\n",
      "62917440\n",
      "Writing Chunk 10\n",
      "62921792\n",
      "61514688\n",
      "Writing Chunk 11\n",
      "61517120\n",
      "60050944\n",
      "Writing Chunk 12\n",
      "60053312\n",
      "58586368\n",
      "Writing Chunk 13\n",
      "58588800\n",
      "57123712\n",
      "Writing Chunk 14\n",
      "57127744\n",
      "55664704\n",
      "Writing Chunk 15\n",
      "55667200\n",
      "54201792\n",
      "Writing Chunk 16\n",
      "54204160\n",
      "52737152\n",
      "Writing Chunk 17\n",
      "52739456\n",
      "51272832\n",
      "Writing Chunk 18\n",
      "51276544\n",
      "49810944\n",
      "Writing Chunk 19\n",
      "49813248\n",
      "48347392\n",
      "Writing Chunk 20\n",
      "48349696\n",
      "46883712\n",
      "Writing Chunk 21\n",
      "46886080\n",
      "45419392\n",
      "Writing Chunk 22\n",
      "45422208\n",
      "43948352\n",
      "Writing Chunk 23\n",
      "43950720\n",
      "42484352\n",
      "Writing Chunk 24\n",
      "42486720\n",
      "41022784\n",
      "Writing Chunk 25\n",
      "41025152\n",
      "39558464\n",
      "Writing Chunk 26\n",
      "39566016\n",
      "38101632\n",
      "Writing Chunk 27\n",
      "38104192\n",
      "36638080\n",
      "Writing Chunk 28\n",
      "36640384\n",
      "35173696\n",
      "Writing Chunk 29\n",
      "35176064\n",
      "33709952\n",
      "Writing Chunk 30\n",
      "33718336\n",
      "32252032\n",
      "Writing Chunk 31\n",
      "32254464\n",
      "30788480\n",
      "Writing Chunk 32\n",
      "30790848\n",
      "29324544\n",
      "Writing Chunk 33\n",
      "29326912\n",
      "27861184\n",
      "Writing Chunk 34\n",
      "27865152\n",
      "26403072\n",
      "Writing Chunk 35\n",
      "26405632\n",
      "24934976\n",
      "Writing Chunk 36\n",
      "24937472\n",
      "23470912\n",
      "Writing Chunk 37\n",
      "23473472\n",
      "22007040\n",
      "Writing Chunk 38\n",
      "22010944\n",
      "20546560\n",
      "Writing Chunk 39\n",
      "20548864\n",
      "19082240\n",
      "Writing Chunk 40\n",
      "19084608\n",
      "17621120\n",
      "Writing Chunk 41\n",
      "17623680\n",
      "16156608\n",
      "Writing Chunk 42\n",
      "16159424\n",
      "14683456\n",
      "Writing Chunk 43\n",
      "14685824\n",
      "13204352\n",
      "Writing Chunk 44\n",
      "13206592\n",
      "11733632\n",
      "Writing Chunk 45\n",
      "11736256\n",
      "10269568\n",
      "Writing Chunk 46\n",
      "10277376\n",
      "8800320\n",
      "Writing Chunk 47\n",
      "8802816\n",
      "7337344\n",
      "Writing Chunk 48\n",
      "7339904\n",
      "5877888\n",
      "Writing Chunk 49\n",
      "5880384\n",
      "4414656\n",
      "Writing Chunk 50\n",
      "4418816\n",
      "2957824\n",
      "Writing Chunk 51\n",
      "2960192\n",
      "1495424\n",
      "Writing Chunk 52\n",
      "1497920\n",
      "154944\n",
      "Writing Chunk 53\n",
      "157504\n",
      "147328\n",
      "Writing Chunk 54\n",
      "149824\n",
      "148096\n",
      "Writing Chunk 55\n",
      "152704\n",
      "148416\n",
      "Writing Chunk 56\n",
      "150784\n",
      "151168\n",
      "Writing Chunk 57\n",
      "158784\n",
      "147712\n",
      "Writing Chunk 58\n",
      "149568\n",
      "148608\n",
      "Writing Chunk 59\n",
      "153408\n",
      "148544\n",
      "Writing Chunk 60\n",
      "151488\n",
      "149696\n",
      "Writing Chunk 61\n",
      "153920\n",
      "148928\n",
      "Writing Chunk 62\n",
      "151424\n",
      "148416\n",
      "Writing Chunk 63\n",
      "150144\n",
      "148608\n",
      "Writing Chunk 64\n",
      "155136\n",
      "148672\n",
      "Writing Chunk 65\n",
      "152192\n",
      "148672\n",
      "Writing Chunk 66\n",
      "150464\n",
      "150528\n",
      "Writing Chunk 67\n",
      "158400\n",
      "148352\n",
      "Writing Chunk 68\n",
      "149888\n",
      "148096\n",
      "Writing Chunk 69\n",
      "150592\n",
      "148864\n",
      "Writing Chunk 70\n",
      "151360\n",
      "147328\n",
      "Writing Chunk 71\n",
      "149952\n",
      "105280\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 16 * 1024 * 1024\n",
    "import time\n",
    "with nifgen.Session(\"Dev1\") as session:\n",
    "    session.output_mode = nifgen.OutputMode.ARB\n",
    "    session.arb_sample_rate = 10_000_000\n",
    "    #160 MB allocation\n",
    "    waveform_handle = session.allocate_waveform(83886080)\n",
    "    session.streaming_waveform_handle = waveform_handle\n",
    "\n",
    "    print(session.streaming_space_available_in_waveform)\n",
    "\n",
    "    # Write 10, 16MB chunks\n",
    "    for i in range(10):\n",
    "        session.write_waveform(waveform_handle, waveform_chunks_float64[i])\n",
    "    # Initiate\n",
    "    session.initiate()\n",
    "\n",
    "    print(session.streaming_space_available_in_waveform)\n",
    "\n",
    "    # Write the rest of the chunks\n",
    "    for i in range(10, len(waveform_chunks_float64)):\n",
    "        print(f'Writing Chunk {i}')\n",
    "        print(session.streaming_space_available_in_waveform)\n",
    "        session.write_waveform(waveform_handle, waveform_chunks_float64[i])\n",
    "        print(session.streaming_space_available_in_waveform)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c152df",
   "metadata": {},
   "source": [
    "Now each pixel has the correct timing. However, it isn't synchronized for the scanning. We need to have it synchronized to the mirror scanner.\n",
    "\n",
    "This means that each row needs a gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3684ac46",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'waveform_normalized' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m session.output_mode = nifgen.OutputMode.ARB\n\u001b[32m      6\u001b[39m session.arb_sample_rate = \u001b[32m100_000_000\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m wf_handle = session.create_waveform(\u001b[43mwaveform_normalized\u001b[49m)\n\u001b[32m      8\u001b[39m session.start_trigger_type = nifgen.StartTriggerType.DIGITAL_EDGE\n\u001b[32m      9\u001b[39m session.digital_edge_start_trigger_edge = nifgen.StartTriggerDigitalEdgeEdge.RISING\n",
      "\u001b[31mNameError\u001b[39m: name 'waveform_normalized' is not defined"
     ]
    }
   ],
   "source": [
    "import nifgen\n",
    "import time\n",
    "\n",
    "with nifgen.Session(\"Dev1\") as session:\n",
    "    session.output_mode = nifgen.OutputMode.ARB\n",
    "    session.arb_sample_rate = 100_000_000\n",
    "    wf_handle = session.create_waveform(waveform_normalized)\n",
    "    session.start_trigger_type = nifgen.StartTriggerType.DIGITAL_EDGE\n",
    "    session.digital_edge_start_trigger_edge = nifgen.StartTriggerDigitalEdgeEdge.RISING\n",
    "    session.trigger_mode = nifgen.TriggerMode.CONTINUOUS\n",
    "    session.digital_edge_start_trigger_source = \"/Dev1/PFI1\"\n",
    "    session.configure_arb_waveform(wf_handle, gain=1, offset=0)\n",
    "    session.initiate()\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc3f068",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
